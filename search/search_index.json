{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tribhuvan University Super Computing","text":""},{"location":"#welcome-to-tu-high-performance-computing-hpc","title":"Welcome to TU High-Performance Computing (HPC)","text":"<p>Tribhuvan University provides a High-Performance Computing (HPC) environment designed to support advanced research and computational workloads. Our HPC infrastructure enables researchers, faculty, and students to run large-scale simulations, machine learning models, and scientific computations efficiently.</p> <p></p>"},{"location":"#what-is-hpc","title":"What is HPC?","text":"<p>High-Performance Computing (HPC) refers to the use of parallel processing to run complex calculations at high speed. TU's HPC cluster consists of powerful computing nodes, GPUs, and optimized software stacks, allowing researchers to process vast amounts of data faster than traditional computing environments.</p>"},{"location":"#key-features-of-tu-hpc","title":"Key Features of TU HPC","text":"<ul> <li>Powerful Compute Nodes \u2013 Multi-core CPU and GPU-based high-performance servers.</li> <li>Parallel Computing \u2013 Run large-scale simulations and computations efficiently.</li> <li>Scalable Storage \u2013 Secure, high-speed data storage for research projects.</li> <li>SLURM Job Scheduler \u2013 Efficient workload management with automated job queuing.</li> <li>AI &amp; Machine Learning Support \u2013 Optimized environment for AI/ML workloads.</li> <li>Scientific Software Stack \u2013 Pre-installed software for research and engineering applications.</li> <li>User Support &amp; Documentation \u2013 Guides, tutorials, and hands-on workshops.</li> </ul>"},{"location":"#why-use-tu-hpc","title":"Why Use TU HPC?","text":"<p>TU HPC is available for faculty, students, and researchers engaged in computationally intensive projects. Our infrastructure supports research in:</p> <ul> <li>Artificial Intelligence &amp; Machine Learning</li> <li>Computational Physics &amp; Chemistry</li> <li>Climate &amp; Environmental Modeling</li> <li>Genomics &amp; Bioinformatics</li> <li>Engineering Simulations</li> <li>Big Data Analytics</li> </ul>"},{"location":"#news-maintenance","title":"News &amp; Maintenance","text":""},{"location":"#latest-news","title":"Latest News","text":"<ul> <li>[March 2025] 4 nodes is down at the cluster.</li> <li>[Feburary 2024] Upcoming HPC training workshop for researchers.</li> <li>[January 2024] Performance upgrade and system optimization.</li> </ul>"},{"location":"#maintenance-schedule","title":"Maintenance Schedule","text":"<ul> <li>March 10, 2025: Scheduled downtime for security updates.</li> <li>April 5, 2025: Hardware maintenance on storage servers.</li> <li>May 15, 2025: Cluster-wide software upgrades.</li> </ul> <p>For real-time updates, visit our Status Page.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#step-1-request-an-hpc-account","title":"Step 1: Request an HPC Account","text":"<p>Access to TU HPC is granted to eligible researchers and students. To request an account, follow the Access Guidelines.</p>"},{"location":"#step-2-connect-to-the-hpc-cluster","title":"Step 2: Connect to the HPC Cluster","text":"<p>After receiving your credentials, connect to the cluster via SSH: <pre><code>ssh username@tu-ip-address\n</code></pre> For detailed instructions, visit the Getting Started Guide.</p>"},{"location":"#step-3-submit-your-first-job","title":"Step 3: Submit Your First Job","text":"<p>Learn how to run jobs using SLURM and optimize computing resources. See SLURM Job Submission for more details.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Access &amp; Accounts \u2013 How to get started with TU HPC.</li> <li>Job Scheduling \u2013 Running and managing jobs with SLURM.</li> <li>Software &amp; Modules \u2013 Using available software and environments.</li> <li>Data Management \u2013 Storing and managing data on HPC.</li> <li>Parallel Computing \u2013 Leveraging parallel computing techniques.</li> <li>Best Practices \u2013 Guidelines for efficient HPC usage.</li> </ul>"},{"location":"#need-help","title":"Need Help?","text":"<p>If you have any questions or require assistance, contact our HPC Support Team at madhav [dot] ghimire@cdp [dot] tu [dot] edu [dot] np</p> <p>Accelerate your research with TU HPC! \ud83d\ude80</p>"},{"location":"about/introduction/","title":"Introduction to Tribhuvan University HPC","text":""},{"location":"about/introduction/#overview","title":"Overview","text":"<p>The High-Performance Computing (HPC) system at Tribhuvan University is a state-of-the-art computational facility established to support scientific research, data-intensive simulations, and advanced computing. The HPC infrastructure is managed by the Central Department of Physics (CDP), Tribhuvan University.</p> <p>The HPC system plays a vital role in accelerating research across multiple domains, including computational physics, material science, artificial intelligence, and climate modeling.</p>"},{"location":"about/introduction/#establishment-and-evolution","title":"Establishment and Evolution","text":"<p>The HPC cluster was initiated on September 1, 2018, with support from the Alexander von Humboldt (AvH) Foundation, Germany, and Higher Education Reform Project (HERP) of TU. The system has since evolved, incorporating multi-core CPU clusters, high-performance GPUs, and optimized software environments for parallel computing.</p>"},{"location":"about/introduction/#objectives-of-tu-hpc","title":"Objectives of TU HPC","text":"<p>The TU HPC infrastructure aims to:</p> <ul> <li>Enable cutting-edge research in computational sciences and engineering.</li> <li>Provide high-performance computational resources for researchers and students.</li> <li>Foster interdisciplinary collaborations with national and international institutions.</li> <li>Train the next generation of researchers in advanced computing techniques.</li> <li>Support large-scale simulations, machine learning, and data analytics.</li> </ul>"},{"location":"about/introduction/#research-domains-supported","title":"Research Domains Supported","text":"<p>The HPC cluster at TU facilitates currently working on research in the following areas:</p> <ul> <li>Condensed Matter Physics: First-principles calculations and quantum simulations.</li> <li>Artificial Intelligence &amp; Machine Learning: Training and deploying deep learning models.</li> <li>Climate and Environmental Modeling: High-resolution weather and climate simulations.</li> <li>Genomics &amp; Bioinformatics: Computational analysis of biological data.</li> <li>Engineering Simulations: Structural, fluid dynamics, and material analysis.</li> <li>Big Data Analytics: Large-scale data processing and visualization.</li> </ul>"},{"location":"about/introduction/#international-collaborations","title":"International Collaborations","text":"<p>TU HPC has established collaborations with leading institutions worldwide, including:</p> <ul> <li>Massachusetts Institute of Technology (MIT), USA</li> <li>University of Texas, USA</li> <li>Leibniz Institute for Solid State and Materials Research (IFW-Dresden), Germany</li> <li>Stanford University, USA</li> <li>Kathmandu University, Nepal</li> </ul> <p>These partnerships have facilitated knowledge exchange, joint research projects, and shared computational resources.</p>"},{"location":"about/introduction/#future-roadmap","title":"Future Roadmap","text":"<p>TU HPC is continuously evolving with plans to:</p> <ul> <li>Expand the computational resources with more GPU nodes and high-memory servers.</li> <li>Enhance data storage capacity and improve network infrastructure.</li> <li>Develop customized HPC training programs for students and researchers.</li> <li>Strengthen collaborations with industries and research labs.</li> </ul> <p>For more details on how to access and utilize TU HPC, visit our Getting Started Guide.</p> <p>Empowering Research with High-Performance Computing! \ud83d\ude80</p>"},{"location":"about/mission/","title":"Mission & Objectives","text":""},{"location":"about/mission/#mission","title":"Mission","text":"<p>The High-Performance Computing (HPC) system at Tribhuvan University is dedicated to advancing scientific research, innovation, and education through high-performance computing resources. Our mission is to empower researchers, educators, and students by providing state-of-the-art computational tools that enhance their ability to solve complex scientific and engineering challenges.</p> <p>Our HPC infrastructure serves as a catalyst for groundbreaking research across multiple disciplines, facilitating high-speed simulations, data-driven discoveries, and technological advancements. We are committed to fostering collaboration, inclusivity, and sustainability in computational research.</p>"},{"location":"about/mission/#objectives","title":"Objectives","text":""},{"location":"about/mission/#1-enhancing-research-capabilities","title":"1. Enhancing Research Capabilities","text":"<ul> <li>Provide a cutting-edge computing environment for faculty, researchers, and students.</li> <li>Support high-performance simulations, machine learning, and big data analytics.</li> <li>Enable cross-disciplinary research collaborations within Tribhuvan University and beyond.</li> </ul>"},{"location":"about/mission/#2-supporting-scientific-and-engineering-innovations","title":"2. Supporting Scientific and Engineering Innovations","text":"<ul> <li>Develop computational solutions for material science, climate modeling, bioinformatics, and artificial intelligence.</li> <li>Facilitate quantum computing and neuromorphic AI research.</li> <li>Assist in complex data analysis and predictive modeling.</li> </ul>"},{"location":"about/mission/#3-promoting-collaboration-and-knowledge-sharing","title":"3. Promoting Collaboration and Knowledge Sharing","text":"<ul> <li>Strengthen partnerships with national and international universities, research institutes, and industries.</li> <li>Establish joint research initiatives with global institutions.</li> <li>Conduct workshops, training programs, and conferences to promote knowledge exchange.</li> </ul>"},{"location":"about/mission/#4-providing-efficient-and-scalable-computational-infrastructure","title":"4. Providing Efficient and Scalable Computational Infrastructure","text":"<ul> <li>Continuously upgrade hardware and software to meet the growing demands of computational research.</li> <li>Expand the compute power with additional GPU nodes and high-memory servers.</li> <li>Ensure efficient job scheduling and resource allocation using SLURM and other workload management tools.</li> </ul>"},{"location":"about/mission/#5-encouraging-sustainable-and-ethical-computing-practices","title":"5. Encouraging Sustainable and Ethical Computing Practices","text":"<ul> <li>Promote energy-efficient computing and green HPC practices.</li> <li>Implement best practices for data security, storage management, and ethical research.</li> <li>Support open-source and reproducible research methodologies.</li> </ul>"},{"location":"about/mission/#6-capacity-building-and-training","title":"6. Capacity Building and Training","text":"<ul> <li>Develop HPC training programs for students, researchers, and faculty members.</li> <li>Provide hands-on tutorials on parallel computing, SLURM job scheduling, and GPU programming.</li> <li>Offer internships and research opportunities to students interested in computational sciences.</li> </ul>"},{"location":"about/mission/#7-facilitating-industry-and-government-collaborations","title":"7. Facilitating Industry and Government Collaborations","text":"<ul> <li>Engage with government organizations and industries to apply HPC in solving real-world challenges.</li> <li>Provide HPC-driven solutions for climate change analysis, urban planning, and healthcare research.</li> <li>Support startups and entrepreneurs utilizing HPC for AI-driven innovation.</li> </ul>"},{"location":"about/mission/#conclusion","title":"Conclusion","text":"<p>Tribhuvan University HPC is committed to providing a world-class computing environment that accelerates research, fosters innovation, and enables transformative discoveries. By continuously evolving, we aim to establish TU HPC as a leading computational research hub in Nepal.</p> <p>Advancing Research &amp; Innovation through High-Performance Computing! \ud83d\ude80</p>"},{"location":"about/research/","title":"Research Applications","text":"<p>The High-Performance Computing (HPC) system at Tribhuvan University serves as a powerful computational resource supporting diverse research domains. Our HPC infrastructure accelerates scientific discoveries, engineering solutions, and data-driven insights by providing scalable and high-speed computing capabilities.</p> <p>We welcome researchers, students, and institutions from all over Nepal to utilize TU HPC for their computational needs. Our facility is open to any field of study that requires advanced computing, simulations, or data analytics.</p>"},{"location":"about/research/#supported-research-domains","title":"Supported Research Domains","text":""},{"location":"about/research/#1-artificial-intelligence-machine-learning","title":"1. Artificial Intelligence &amp; Machine Learning","text":"<ul> <li>Training deep learning models using GPU-accelerated computing.</li> <li>AI applications in healthcare, finance, agriculture, and robotics.</li> <li>Development of Spiking Neural Networks (SNNs) and Quantum Machine Learning (QML).</li> </ul>"},{"location":"about/research/#2-computational-physics-material-science","title":"2. Computational Physics &amp; Material Science","text":"<ul> <li>First-principles calculations (DFT, Quantum Espresso, VASP).</li> <li>Simulation of electronic, magnetic, and optical properties of materials.</li> <li>Research on quantum materials, topological insulators, and perovskites.</li> </ul>"},{"location":"about/research/#3-climate-environmental-modeling","title":"3. Climate &amp; Environmental Modeling","text":"<ul> <li>High-resolution weather forecasting and climate simulations.</li> <li>Predicting air pollution levels, water resource management, and disaster modeling.</li> <li>Simulating hydrological and atmospheric processes.</li> </ul>"},{"location":"about/research/#4-genomics-bioinformatics","title":"4. Genomics &amp; Bioinformatics","text":"<ul> <li>Large-scale DNA/RNA sequence analysis.</li> <li>Computational drug discovery and molecular docking.</li> <li>Epidemic and pandemic modeling (e.g., COVID-19 simulations).</li> </ul>"},{"location":"about/research/#5-big-data-analytics-computational-social-sciences","title":"5. Big Data Analytics &amp; Computational Social Sciences","text":"<ul> <li>Processing large datasets for economic, political, and social research.</li> <li>Sentiment analysis, social media data mining, and behavioral modeling.</li> <li>Predictive analytics for policy planning and governance.</li> </ul>"},{"location":"about/research/#6-computational-chemistry-quantum-simulations","title":"6. Computational Chemistry &amp; Quantum Simulations","text":"<ul> <li>Molecular dynamics and chemical reaction modeling.</li> <li>Quantum mechanical simulations for novel material discovery.</li> <li>Quantum computing algorithms applied to chemical and biological problems.</li> </ul>"},{"location":"about/research/#7-engineering-computational-fluid-dynamics-cfd","title":"7. Engineering &amp; Computational Fluid Dynamics (CFD)","text":"<ul> <li>Structural analysis and finite element modeling (FEM).</li> <li>CFD simulations for aerospace, automotive, and hydropower industries.</li> <li>Multiphysics simulations combining fluid dynamics, heat transfer, and stress analysis.</li> </ul>"},{"location":"about/research/#8-astrophysics-space-science","title":"8. Astrophysics &amp; Space Science","text":"<ul> <li>Simulating gravitational waves and black hole dynamics.</li> <li>Analyzing astronomical datasets and cosmological models.</li> <li>Computational modeling of stellar evolution and planetary atmospheres.</li> </ul>"},{"location":"about/research/#9-neuroscience-brain-simulations","title":"9. Neuroscience &amp; Brain Simulations","text":"<ul> <li>Large-scale neural network simulations and brain modeling.</li> <li>Understanding cognitive processes using computational neuroscience.</li> <li>Research on neuromorphic computing and brain-inspired AI.</li> </ul>"},{"location":"about/research/#open-for-all-research-fields","title":"Open for All Research Fields","text":"<p>TU HPC is committed to supporting interdisciplinary research. We welcome:</p> <ul> <li>Researchers from all universities and institutions in Nepal.</li> <li>Independent researchers and Ph.D. scholars working on computational projects.</li> <li>Government and industry collaborations seeking HPC-powered solutions.</li> <li>New research domains that require computational resources.</li> </ul> <p>If your research requires high-performance computing, you are encouraged to apply for access. For more details on how to get started, visit our Access &amp; Accounts page.</p> <p>Enabling research in every field, for every researcher in Nepal! \ud83d\ude80</p>"},{"location":"about/team/","title":"HPC Team","text":"<p>The High-Performance Computing (HPC) infrastructure at Tribhuvan University is managed by a dedicated team of researchers and technical experts. The team ensures seamless operation, maintenance, and continuous development of the HPC system to support cutting-edge research in Nepal.</p>"},{"location":"about/team/#hpc-management-team","title":"HPC Management Team","text":"<p>The following individuals and research groups oversee the administration, support, and expansion of TU HPC:</p>"},{"location":"about/team/#dr-madhav-prasad-ghimire-principal-investigator","title":"Dr. Madhav Prasad Ghimire (Principal Investigator)","text":"<ul> <li>Associate Professor, Central Department of Physics, Tribhuvan University</li> <li>Leads HPC infrastructure development and research activities.</li> <li>Specializes in computational physics, material science, and quantum simulations.</li> <li>Established collaborations with MIT, IFW-Dresden, and Stanford University.</li> </ul>"},{"location":"about/team/#dr-niraj-dhital-co-principal-investigator","title":"Dr. Niraj Dhital (Co-Principal Investigator)","text":"<ul> <li>Assistant Professor, Tribhuvan University.</li> <li>Research expertise in computational physics, material science, and density functional theory (DFT).</li> <li>Manages job scheduling, SLURM configurations, and system optimization.</li> <li>Provides technical support and training to HPC users.</li> <li>Active in research collaborations with international institutions and publishing in high-impact journals.</li> </ul>"},{"location":"about/team/#aatiz-ghimire-hpc-engineer-researcher","title":"Aatiz Ghimire (HPC Engineer &amp; Researcher)","text":"<ul> <li>Specializes in high-performance computing, AI, and quantum computing.</li> <li>Manages software stack optimization, GPU acceleration, and user access.</li> <li>Supports new research projects utilizing HPC infrastructure.</li> </ul>"},{"location":"about/team/#advanced-materials-research-laboratory-amrl-team","title":"Advanced Materials Research Laboratory (AMRL) Team","text":"<p>The AMRL team, based at the Central Department of Physics, is responsible for: - Maintaining and expanding the HPC infrastructure. - Conducting research in condensed matter physics, computational materials, and AI. - Collaborating with national and international institutions for HPC-driven projects. - Organizing workshops and training programs to educate students and researchers in HPC technologies.</p>"},{"location":"about/team/#contact-the-hpc-team","title":"Contact the HPC Team","text":"<p>For inquiries, support, or collaboration opportunities, please contact:</p> <ul> <li>Dr. Madhav Prasad Ghimire \u2013 madhav [dot] ghimire@cdp [dot] tu [dot] edu [dot] np</li> <li>Niraj Dhital \u2013 niraj [dot] dhital@tu [dot] edu [dot] np</li> <li>Aatiz Ghimire \u2013 aatiz [dot] 795522@sms [dot] tu [dot] edu[dot] np</li> </ul> <p>The TU HPC Team is committed to fostering research and innovation in Nepal! \ud83d\ude80</p>"},{"location":"access/registration/","title":"Account Registration","text":"<p>Access to the Tribhuvan University HPC system is granted to researchers, faculty, students, and industry collaborators who require high-performance computing resources for scientific research and computational projects. The registration process ensures that users have a valid research proposal and appropriate infrastructure needs.</p>"},{"location":"access/registration/#registration-process","title":"Registration Process","text":"<p>To request access to TU HPC, follow these steps:</p>"},{"location":"access/registration/#1-prepare-a-research-proposal","title":"1. Prepare a Research Proposal","text":"<p>Applicants must submit a detailed research proposal that includes:</p> <ul> <li>Research title and objectives</li> <li>Computational requirements (CPU, GPU, software dependencies, storage needs, etc.)</li> <li>Expected duration of HPC usage</li> <li>Potential research impact and outcomes</li> </ul>"},{"location":"access/registration/#2-recommendation-letter-for-undergraduate-students","title":"2. Recommendation Letter (For Undergraduate Students)","text":"<p>Undergraduate students must provide a recommendation letter from:</p> <ul> <li>A faculty advisor or supervisor supporting their research project.</li> <li>The letter should highlight the student\u2019s capability and need for HPC resources.</li> </ul>"},{"location":"access/registration/#3-send-an-email-request","title":"3. Send an Email Request","text":"<p>Applicants should email their request to Dr. Madhav Prasad Ghimire at:</p> <p>\ud83d\udce9 madhav [dot] ghimire @ cdp [dot] tu [dot] edu [dot] np</p> <p>The email should include:</p> <ul> <li>Research proposal (PDF format)</li> <li>Recommendation letter (if applicable)</li> <li>Requested infrastructure details</li> </ul>"},{"location":"access/registration/#4-review-approval","title":"4. Review &amp; Approval","text":"<ul> <li>The HPC administration team will review applications.</li> <li>Approved users will receive an HPC account and login credentials.</li> <li>Users must agree to the HPC usage policies before accessing the system.</li> </ul>"},{"location":"access/registration/#important-notes","title":"Important Notes","text":"<ul> <li>Only approved research projects will be granted access.</li> <li>HPC resources are limited, so users must request only what they require.</li> <li>Users must renew access periodically based on project requirements.</li> </ul> <p>For any inquiries or support, contact madhav [dot] ghimire @ cdp [dot] tu [dot] edu [dot] np.</p> <p>Enabling researchers with high-performance computing power! \ud83d\ude80</p>"},{"location":"access/roles/","title":"User Roles & Permissions","text":""},{"location":"access/roles/#user-roles","title":"User Roles","text":"<p>The following user roles define access levels and responsibilities within the HPC system:</p>"},{"location":"access/roles/#1-hpc-administrators","title":"1. HPC Administrators","text":"<ul> <li>Responsible for system management, maintenance, and user support.</li> <li>Have full access to all system configurations, job scheduling, and software installations.</li> <li>Manage network security, storage, and system upgrades.</li> </ul>"},{"location":"access/roles/#2-faculty-principal-investigators-pis","title":"2. Faculty &amp; Principal Investigators (PIs)","text":"<ul> <li>Can request access for research projects and student users.</li> <li>Have priority access to HPC resources for approved projects.</li> <li>Allowed to install custom software upon request.</li> <li>Responsible for ensuring ethical usage of HPC resources.</li> </ul>"},{"location":"access/roles/#3-research-scholars-graduate-students","title":"3. Research Scholars &amp; Graduate Students","text":"<ul> <li>Can submit jobs to compute nodes and use available software.</li> <li>Allowed to use scratch storage for temporary data processing.</li> <li>Must follow fair usage policies to ensure equitable resource distribution.</li> </ul>"},{"location":"access/roles/#4-undergraduate-students","title":"4. Undergraduate Students","text":"<ul> <li>Allowed access only under supervision of a faculty member or PI.</li> <li>Required to submit a recommendation letter to gain access.</li> <li>Can perform basic computations and coursework-related HPC tasks.</li> </ul>"},{"location":"access/roles/#5-external-collaborators-industry-users","title":"5. External Collaborators &amp; Industry Users","text":"<ul> <li>Can access HPC resources through joint research projects.</li> <li>Require prior approval from the HPC administration team.</li> <li>Access is time-limited and subject to specific project needs.</li> </ul>"},{"location":"access/roles/#permissions-access-levels","title":"Permissions &amp; Access Levels","text":"<p>The table below outlines the access privileges for each user role:</p> Permission Admins Faculty/PIs Researchers Undergraduates Industry Users Submit Jobs to Compute Nodes \u2705 \u2705 \u2705 \u2705 (Limited) \u2705 (Limited) Access Scratch Storage \u2705 \u2705 \u2705 \u2705 (Limited) \u2705 Install Custom Software \u2705 \u2705 (Approval Required) \u274c \u274c \u274c Modify SLURM Job Scheduler \u2705 \u274c \u274c \u274c \u274c Request Additional Resources \u2705 \u2705 \u2705 (Limited) \u274c \u2705 Long-term Storage Allocation \u2705 \u2705 \u2705 (Approval Required) \u274c \u2705"},{"location":"access/roles/#policy-compliance-violations","title":"Policy Compliance &amp; Violations","text":"<p>Users must comply with HPC policies regarding fair usage, security, and data privacy. Violations such as misuse of resources, unauthorized software installations, or security breaches may result in account suspension or permanent revocation of access.</p> <p>Ensuring responsible and efficient use of HPC resources! \ud83d\ude80</p>"},{"location":"access/security/","title":"Security Guidelines","text":"<p>The HPC system at Tribhuvan University follows strict security policies to ensure data integrity, system stability, and secure access. All users must follow these security guidelines to maintain a safe computing environment.</p>"},{"location":"access/security/#1-user-authentication-access-control","title":"1. User Authentication &amp; Access Control","text":"<ul> <li>SSH Key-Based Authentication: Users must use SSH keys for remote access; password-based authentication is disabled.</li> <li>Multi-Factor Authentication (MFA): If enabled, users must follow the additional authentication steps.</li> <li>User Account Management:</li> <li>Each user must have a unique account.</li> <li>Sharing credentials is strictly prohibited.</li> <li>Users must update their passwords regularly and use strong password policies.</li> </ul>"},{"location":"access/security/#2-data-security-privacy","title":"2. Data Security &amp; Privacy","text":"<ul> <li>Users must not store sensitive personal data or proprietary research data without encryption.</li> <li>All data must be stored in designated directories (<code>/home</code>, <code>/mnt/storage0</code>) following storage policies.</li> <li>Data backups are the responsibility of the users; the HPC system provides limited backup capabilities.</li> <li>Users should not share or transfer research data without appropriate permissions.</li> </ul>"},{"location":"access/security/#3-network-remote-access-security","title":"3. Network &amp; Remote Access Security","text":"<ul> <li>The HPC system is hosted at ITIC Tribhuvan University, and NAT is managed by TU ITIC and the Registrar Office.</li> <li>Only authorized users can access the system remotely via SSH.</li> <li>Public-facing services are restricted; users cannot expose any services without administrator approval.</li> <li>No unauthorized VPNs, proxies, or tunneling methods should be used.</li> </ul>"},{"location":"access/security/#4-software-installation-execution-policies","title":"4. Software Installation &amp; Execution Policies","text":"<ul> <li>Users cannot install system-level software without approval.</li> <li>All software requests should be made via [madhav [dot] ghimire @ cdp [dot] tu [dot] edu [dot] np].</li> <li>Executable files must be scanned for security risks before running.</li> <li>Users must not run scripts that consume excessive resources or cause instability.</li> </ul>"},{"location":"access/security/#5-job-submission-fair-usage","title":"5. Job Submission &amp; Fair Usage","text":"<ul> <li>Users must ensure efficient job scheduling and avoid unnecessary resource hogging.</li> <li>Long-running jobs must be checked and optimized to minimize impact on shared resources.</li> <li>Priority scheduling is based on fair usage policies; abusing the system may result in restricted access.</li> </ul>"},{"location":"access/security/#6-incident-reporting-policy-violations","title":"6. Incident Reporting &amp; Policy Violations","text":"<ul> <li>Users must report security incidents or suspicious activities immediately to support.</li> <li>Violations of security policies may result in temporary or permanent suspension.</li> <li>Regular audits are conducted to monitor compliance and ensure the integrity of the HPC system.</li> </ul>"},{"location":"access/security/#7-future-security-enhancements","title":"7. Future Security Enhancements","text":"<ul> <li>Implementation of enhanced network monitoring tools.</li> <li>Regular security awareness training for users.</li> <li>Exploring VPN-based secure access for external researchers.</li> </ul> <p>For any security concerns or questions, contact madhav [dot] ghimire @ cdp [dot] tu [dot] edu [dot] np.</p> <p>Ensuring a secure and high-performance computing environment! \ud83d\udd12\ud83d\ude80</p>"},{"location":"access/who_can_access/","title":"Who Can Access?","text":"<p>The HPC system at Tribhuvan University is designed to support researchers, faculty, and students engaged in computational research. Access to the HPC infrastructure is available to national and international collaborators, subject to approval by the TU HPC administration.</p>"},{"location":"access/who_can_access/#eligible-users","title":"Eligible Users","text":"<p>The following groups are eligible to apply for access to TU HPC:</p>"},{"location":"access/who_can_access/#1-faculty-researchers","title":"1. Faculty &amp; Researchers","text":"<ul> <li>Professors and researchers affiliated with Tribhuvan University and other academic institutions in Nepal.</li> <li>Researchers from national and international universities collaborating with TU.</li> <li>Scientists working on HPC-based projects in material science, AI, computational physics, and related fields from Nepal.</li> </ul>"},{"location":"access/who_can_access/#2-students-graduate-undergraduate","title":"2. Students (Graduate &amp; Undergraduate)","text":"<ul> <li>Master\u2019s and Ph.D. students conducting research that requires high-performance computing.</li> <li>Undergraduate students participating in HPC-related coursework, projects, or internships.</li> <li>Students must be supervised by an approved faculty advisor or research group.</li> </ul>"},{"location":"access/who_can_access/#3-government-industry-collaborators","title":"3. Government &amp; Industry Collaborators","text":"<ul> <li>Researchers from government organizations requiring computational resources for policy planning, climate modeling, or engineering simulations.</li> <li>Industry partners and startups engaged in AI research, data analytics, and scientific computing.</li> <li>Collaborators involved in joint projects with TU HPC or AMRL research groups.</li> </ul>"},{"location":"access/who_can_access/#4-independent-international-researchers","title":"4. Independent &amp; International Researchers","text":"<ul> <li>Independent researchers may request access for specific, well-defined projects from Nepalese Origin.</li> <li>International collaborations are welcome, subject to approval and partnership agreements.</li> </ul>"},{"location":"access/who_can_access/#how-to-apply-for-access","title":"How to Apply for Access","text":"<p>To gain access to the TU HPC system, users must:</p> <ol> <li>Submit an HPC Access Request</li> <li>Fill out the application form available at TU HPC Registration.</li> <li> <p>Include details of your research project and computing requirements.</p> </li> <li> <p>Approval &amp; Account Creation</p> </li> <li>Applications will be reviewed by the HPC administration team.</li> <li> <p>Approved users will receive login credentials and access instructions.</p> </li> <li> <p>Comply with HPC Usage Policies</p> </li> <li>Users must adhere to the HPC code of conduct and fair usage policies.</li> <li>Violations of policies may result in account suspension.</li> </ol>"},{"location":"access/who_can_access/#acknowledging-tu-hpc-in-research","title":"Acknowledging TU HPC in Research","text":"<p>Users publishing research that utilizes TU HPC resources must acknowledge the infrastructure in their publications:</p> <p>\"This research acknowledges IFW-Dresden for providing the large-scale compute nodes to Tribhuvan University for scientific computations.\"</p> <p>For more details on applying for access, visit the HPC Access Guide or contact madhav [dot] ghimire @ cdp [dot] tu [dot] edu [dot] np.</p> <p>Providing equitable access to high-performance computing for Nepal's research community! \ud83d\ude80</p>"},{"location":"getting_started/best_practices/","title":"Best Practices","text":""},{"location":"getting_started/best_practices/#best-practice-for-using-hpc","title":"Best Practice for using HPC","text":"<ul> <li> <p>Do NOT run any job which is longer than few minutes on the login nodes. Login node is for compilation of job. It is best to run the job on computes. (compute nodes)</p> </li> <li> <p>Do NOT store your data under /home/user-name. There should be a storage0 or similar folder. Use the designated storage under your home account for storing files.</p> </li> <li></li> </ul>"},{"location":"getting_started/file_transfer/","title":"File Transfers","text":"<p>Users working on TU HPC need to transfer files between their local systems and the HPC cluster. Globus is not available, so users must use standard secure file transfer methods like <code>scp</code>, <code>rsync</code>, and <code>sftp</code> for safe and efficient data transfer.</p>"},{"location":"getting_started/file_transfer/#1-secure-file-transfer-using-scp","title":"1. Secure File Transfer Using SCP","text":"<p>The <code>scp</code> (Secure Copy Protocol) command allows users to copy files securely over SSH.</p>"},{"location":"getting_started/file_transfer/#uploading-a-file-to-hpc","title":"Uploading a File to HPC","text":"<p>From your local machine, use: <pre><code>scp myfile.txt username@tu-hpc-ip:/home/username/\n</code></pre> This copies <code>myfile.txt</code> from your local system to your home directory on TU HPC.</p>"},{"location":"getting_started/file_transfer/#downloading-a-file-from-hpc","title":"Downloading a File from HPC","text":"<p><pre><code>scp username@tu-hpc-ip:/home/username/myfile.txt ./\n</code></pre> This downloads <code>myfile.txt</code> from the HPC system to your local system.</p>"},{"location":"getting_started/file_transfer/#transferring-entire-directories","title":"Transferring Entire Directories","text":"<p><pre><code>scp -r myfolder username@tu-hpc-ip:/home/username/\n</code></pre> The <code>-r</code> flag copies entire directories recursively.</p>"},{"location":"getting_started/file_transfer/#2-efficient-file-transfer-with-rsync","title":"2. Efficient File Transfer with Rsync","text":"<p>For resumable and optimized file transfers, <code>rsync</code> is recommended.</p>"},{"location":"getting_started/file_transfer/#uploading-a-directory-with-rsync","title":"Uploading a Directory with Rsync","text":"<p><pre><code>rsync -avz myfolder username@tu-hpc-ip:/home/username/\n</code></pre> The <code>-a</code> (archive), <code>-v</code> (verbose), and <code>-z</code> (compression) flags ensure efficient transfer.</p>"},{"location":"getting_started/file_transfer/#downloading-files-using-rsync","title":"Downloading Files Using Rsync","text":"<p><pre><code>rsync -avz username@tu-hpc-ip:/home/username/myfolder ./\n</code></pre> If a transfer is interrupted, simply rerun the same command to resume from where it left off.</p>"},{"location":"getting_started/file_transfer/#3-interactive-file-transfer-with-sftp","title":"3. Interactive File Transfer with SFTP","text":"<p>For interactive file transfers, use <code>sftp</code>: <pre><code>sftp username@tu-hpc-ip\n</code></pre> Once connected, use: - <code>put myfile.txt</code> (Upload file to HPC) - <code>get myfile.txt</code> (Download file from HPC) - <code>ls</code> (List remote files) - <code>bye</code> (Exit SFTP session)</p>"},{"location":"getting_started/file_transfer/#4-large-file-transfers-storage-guidelines","title":"4. Large File Transfers &amp; Storage Guidelines","text":"<ul> <li>Use Rsync for large datasets to avoid re-transfers of unchanged files.</li> <li>Store files in appropriate directories (<code>/home/username/</code> for personal data, <code>/mnt/storage0/</code> for temporary large-scale computations).</li> <li>Do not store unnecessary files in <code>/home/</code>, as space is limited.</li> </ul>"},{"location":"getting_started/file_transfer/#5-automating-file-transfers","title":"5. Automating File Transfers","text":"<p>To automate regular file transfers, add your SSH key for passwordless authentication and create a cron job: <pre><code>crontab -e\n</code></pre> Example cron job to sync a folder daily at midnight: <pre><code>0 0 * * * rsync -avz /local/folder username@tu-hpc-ip:/home/username/\n</code></pre></p>"},{"location":"getting_started/file_transfer/#6-troubleshooting-file-transfers","title":"6. Troubleshooting File Transfers","text":"<ul> <li>If <code>scp</code> or <code>rsync</code> is slow, try adding <code>-e \"ssh -C\"</code> for compression: <pre><code>scp -C myfile.txt username@tu-hpc-ip:/home/username/\n</code></pre></li> <li>If the connection fails, ensure your SSH keys are correctly set up.</li> <li>Check available space using: <pre><code>df -h\n</code></pre></li> </ul> <p>Efficient and secure file transfer for HPC workflows! \ud83d\ude80</p>"},{"location":"getting_started/first_job/","title":"Running First Job","text":"<p>After gaining access to the HPC system at Tribhuvan University, users can submit jobs using the SLURM job scheduler. This guide explains how to run your first job, monitor its status, and optimize job submissions.</p>"},{"location":"getting_started/first_job/#1-logging-into-the-hpc-system","title":"1. Logging into the HPC System","text":"<p>Before running jobs, connect to the TU HPC system via SSH: <pre><code>ssh username@tu-hpc-ip\n</code></pre> Ensure you are in your home directory or an appropriate project folder before submitting jobs.</p>"},{"location":"getting_started/first_job/#2-creating-a-slurm-job-script","title":"2. Creating a SLURM Job Script","text":"<p>A job script tells the SLURM scheduler how to allocate resources and execute your program. Create a job script (e.g., <code>job.slurm</code>) using a text editor: <pre><code>nano job.slurm\n</code></pre> Add the following SLURM directives to specify job parameters: <pre><code>#!/bin/bash\n#SBATCH --job-name=test_job  # Job name\n#SBATCH --output=output.txt  # Output file\n#SBATCH --error=error.log    # Error file\n#SBATCH --ntasks=1           # Number of tasks\n#SBATCH --cpus-per-task=4    # Number of CPU cores per task\n#SBATCH --time=01:00:00      # Time limit (hh:mm:ss)\n#SBATCH --partition=normal   # Specify partition (e.g., normal, protein, fplo)\n\nmodule load python/3.10.12   # Load required module\npython my_script.py          # Run Python script\n</code></pre> Save and exit (<code>CTRL + X</code>, then <code>Y</code> and <code>ENTER</code>).</p>"},{"location":"getting_started/first_job/#3-submitting-the-job","title":"3. Submitting the Job","text":"<p>Submit the job to the SLURM scheduler using: <pre><code>sbatch job.slurm\n</code></pre> This will add your job to the queue, and it will execute once resources are available.</p>"},{"location":"getting_started/first_job/#4-monitoring-job-status","title":"4. Monitoring Job Status","text":"<p>To check the status of your submitted jobs, use: <pre><code>squeue -u $USER\n</code></pre> To view detailed job information: <pre><code>scontrol show job &lt;job_id&gt;\n</code></pre> To cancel a job: <pre><code>scancel &lt;job_id&gt;\n</code></pre></p>"},{"location":"getting_started/first_job/#5-checking-job-output","title":"5. Checking Job Output","text":"<p>Once the job is completed, check the output and error files: <pre><code>cat output.txt  # View job output\ncat error.log   # View error messages (if any)\n</code></pre></p>"},{"location":"getting_started/first_job/#6-optimizing-job-submissions","title":"6. Optimizing Job Submissions","text":"<ul> <li>Use appropriate resource allocation (CPU, memory, and time) to avoid long queue times.</li> <li>Run test jobs with small datasets before scaling up computations.</li> <li>Use job dependencies if running multiple interdependent tasks: <pre><code>sbatch --dependency=afterok:&lt;job_id&gt; job2.slurm\n</code></pre></li> <li>Consider using parallel computing (MPI, OpenMP) for efficiency.</li> </ul>"},{"location":"getting_started/first_job/#7-example-running-a-gpu-job","title":"7. Example: Running a GPU Job","text":"<p>For GPU-based workloads, modify the script as follows: <pre><code>#!/bin/bash\n#SBATCH --job-name=gpu_test\n#SBATCH --output=gpu_output.txt\n#SBATCH --gres=gpu:1        # Request 1 GPU\n#SBATCH --partition=protein # Use GPU-enabled partition\n\nmodule load cuda/11.2\n./gpu_program\n</code></pre> Submit it using:</p> <pre><code>sbatch gpu_job.slurm\n</code></pre> <p>Start computing efficiently on TU HPC! \ud83d\ude80</p>"},{"location":"getting_started/ssh/","title":"Accessing the System","text":""},{"location":"getting_started/ssh/#remote-system-access","title":"Remote System Access","text":"<ul> <li> <p>To access Super Computing Facility of Tribhuvan University you need to \u201cssh\u201d the login server.</p> </li> <li> <p>The login node is primary gateway to the rest of the cluster, which has a job scheduler (called Slurm). You may submit jobs to the queue and they will run when the required resources are available.</p> </li> <li> <p>Please do not run programs directly on login node. Login node is use to submit jobs, transfer data and to compile source code. (If your compilation takes more than a few minutes, you should submit the compilation job into the queue to be run on the cluster.)</p> </li> <li> <p>Under your /home/user directory there is a storage0 or simialar. Use this folder to hold the store the data required for runnung the job.</p> </li> </ul>"},{"location":"getting_started/ssh/#remote-access-from-windows-desktop","title":"Remote access from Windows Desktop","text":"<p>PuTTY is the most popular open source \u201cssh\u201d client application for Windows, you can Download it from here!. Once installed, find the PuTTY application shortcut in your Start Menu, desktop. On clicking the PuTTY icon The PuTTY Configuration dialog should appear. Locate the \u201cHost Name or IP Address\u201d input Field in the PuTTY Configuration screen. Enter the user name along with IP address or Hostname with which you wish to connect.</p> <p>(e.g. [username]@hpc.tu.edu.np)</p> <p>Note: remove the '[    ]' from username.</p> <p>Enter your password when prompted, and press Enter.</p>"},{"location":"getting_started/ssh/#remote-access-from-linux-desktop-mac","title":"Remote access from Linux Desktop / Mac","text":"<p>Both Mac and Linux systems provide a built-in SSH client, so there is no need to install any additional package. Open the terminal, connect to an SSH server by typing the following command: </p> <p><code>ssh -X [username]@[hostname]</code> </p> <p>For example, to connect to the Super Computing Facility of Tribhuvan University Login Node, with the username</p> <p><code>user: ssh -X user1@hpc.tu.edu.np</code></p> <p>You will be prompted for a password, and then will be connected to the server.</p>"},{"location":"getting_started/ssh/#first-login","title":"First login","text":"<ul> <li>Whenever the newly created user on Super Computing Facility of Tribhuvan University, tries to login with the user Id and password (temporary, system generated) provided over the Email through ICTC support, he/she should change the to new password.</li> </ul> <p>Given next is a screenshot that describes the scenario for \u201cfirst login\u201d </p> <p>Type : yes, otherwise the system will disconnet.</p> <ul> <li>It is strongly recommended to change password after first login.</li> </ul>"},{"location":"getting_started/ssh/#change-password","title":"Change Password","text":"<ul> <li> <p>It is recommended that you have a strong password which contains the combination of alphabets (lower case / upper case), numbers, and a few special characters that you can easily remember.</p> </li> <li> <p>To change the password, Use the <code>passwd</code> command to change the password for the user from login node.</p> </li> </ul> <p></p> <p>Note: The typed password is invisible in terminal.</p> <p>Enabling researchers with high-performance computing power! \ud83d\ude80</p>"},{"location":"resources/compute/","title":"Compute Infrastructure","text":""},{"location":"resources/compute/#hpc-cluster-architecture","title":"HPC Cluster Architecture","text":"<p>TU HPC is structured into multiple compute nodes, ensuring scalability and efficiency. The key components of the infrastructure include:</p>"},{"location":"resources/compute/#1-compute-nodes","title":"1. Compute Nodes","text":"<p>TU HPC consists of multiple compute nodes, categorized into different partitions:</p> <ul> <li>Normal Partition: General-purpose nodes for CPU-based computing.</li> <li>Protein Partition: Dedicated GPU node for high-performance computing.</li> <li>FPLO Partition: Nodes configured for specific research applications.</li> </ul>"},{"location":"resources/compute/#node-configuration","title":"Node Configuration","text":"<ul> <li>Total Compute Nodes: 10</li> <li>CPU Architecture: x86_64</li> <li>Processor Model: Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz</li> <li>Total CPU Cores: 20 per node (10 cores per socket, 2 sockets per node)</li> <li>Threading: 1 thread per core</li> <li>NUMA Nodes: 2 (Ensuring optimized memory access)</li> </ul>"},{"location":"resources/compute/#2-job-scheduling-system","title":"2. Job Scheduling System","text":"<ul> <li>SLURM-based workload manager for efficient job scheduling and resource allocation.</li> <li>Supports batch and interactive job submission.</li> <li>Users can specify CPU, GPU, memory, and time requirements for optimal usage.</li> </ul>"},{"location":"resources/compute/#3-gpu-infrastructure","title":"3. GPU Infrastructure","text":"<ul> <li>Dedicated GPU Node: Available under the Protein Partition.</li> <li>Supports CUDA and OpenCL-based parallel computing.</li> <li>Optimized for deep learning, molecular dynamics, and AI research.</li> </ul>"},{"location":"resources/compute/#4-storage-file-systems","title":"4. Storage &amp; File Systems","text":"<p>TU HPC provides a multi-tiered storage system to support computational research:</p> <ul> <li>Root Storage: <code>/dev/mapper/centos-root</code> \u2013 50GB (OS &amp; system files)</li> <li>Software Storage: <code>/dev/sdb1</code> \u2013 470GB (Installed software and modules)</li> <li>Scratch Storage: <code>/mnt/storage0</code> \u2013 1.9TB (High-speed temporary storage)</li> <li>Home Directory: <code>/home</code> \u2013 1.8TB (User personal storage)</li> <li>Memory File System: <code>tmpfs</code> \u2013 126GB (High-speed temporary memory storage)</li> </ul>"},{"location":"resources/compute/#5-networking-connectivity","title":"5. Networking &amp; Connectivity","text":"<ul> <li>High-speed interconnect for low-latency data transfer between nodes.</li> <li>Secure SSH access for remote login.</li> </ul>"},{"location":"resources/compute/#performance-scalability","title":"Performance &amp; Scalability","text":"<p>The HPC infrastructure at TU is designed to support high-throughput computing (HTC) and massively parallel processing (MPP). It provides:</p> <ul> <li>Scalability to accommodate growing computational demands.</li> <li>Resource optimization to maximize job efficiency.</li> <li>Flexible configurations for different research requirements.</li> </ul>"},{"location":"resources/compute/#supported-workloads","title":"Supported Workloads","text":"<p>The compute infrastructure supports a wide range of research activities, including:</p> <ul> <li>Computational Physics &amp; Chemistry</li> <li>Machine Learning &amp; Deep Learning</li> <li>Molecular Dynamics Simulations</li> <li>Weather &amp; Climate Modeling</li> <li>Structural Engineering &amp; Finite Element Analysis</li> <li>High-Performance Data Analytics</li> </ul>"},{"location":"resources/compute/#future-upgrades","title":"Future Upgrades","text":"<p>TU HPC is continuously expanding with:</p> <ul> <li>More GPU nodes for AI and scientific computing.</li> <li>Increased storage capacity for big data applications.</li> <li>Enhanced networking and security to ensure seamless operations.</li> </ul> <p>For more details on how to access and utilize TU HPC resources, visit the Getting Started Guide.</p> <p>Empowering research with high-performance computing! \ud83d\ude80</p>"},{"location":"resources/network/","title":"Network & Security","text":"<p>The High-Performance Computing (HPC) system at Tribhuvan University is hosted at the Information Technology Innovation Center (ITIC), Tribhuvan University. The system ensures secure, reliable, and high-speed connectivity for researchers accessing computational resources remotely or within the university network.</p>"},{"location":"resources/network/#network-architecture","title":"Network Architecture","text":"<p>The TU HPC network infrastructure is built for efficient data communication and secure remote access. It consists of:</p>"},{"location":"resources/network/#1-hosting-connectivity","title":"1. Hosting &amp; Connectivity","text":"<ul> <li>The HPC system is physically hosted at TU ITIC.</li> <li>Network Address Translation (NAT) is managed by TU ITIC and the TU Registrar Office.</li> <li>Connectivity is based on a standard Ethernet network, not Infiniband.</li> </ul>"},{"location":"resources/network/#2-external-internal-access","title":"2. External &amp; Internal Access","text":"<ul> <li>Remote Access: Users connect to the HPC system via SSH (Secure Shell).</li> <li>Internal Network: Compute nodes communicate over a local Ethernet network.</li> </ul>"},{"location":"resources/network/#3-data-transfer","title":"3. Data Transfer","text":"<ul> <li>Secure file transfer via SCP, SFTP, and Rsync.</li> <li>Scratch storage for temporary data transfer optimization.</li> </ul>"},{"location":"resources/network/#security-measures","title":"Security Measures","text":"<p>TU HPC implements multiple security policies to protect user data and system integrity:</p>"},{"location":"resources/network/#1-user-authentication-access-control","title":"1. User Authentication &amp; Access Control","text":"<ul> <li>Centralized authentication system to manage user credentials.</li> <li>SSH key-based authentication for secure remote access.</li> <li>Role-based access control (RBAC) to restrict administrative privileges.</li> </ul>"},{"location":"resources/network/#2-firewall-network-security","title":"2. Firewall &amp; Network Security","text":"<ul> <li>Firewall rules managed by TU ITIC to restrict unauthorized access.</li> <li>Network traffic monitoring to detect and mitigate suspicious activity.</li> <li>Isolation of compute nodes from direct external access for security.</li> </ul>"},{"location":"resources/network/#3-data-protection-compliance","title":"3. Data Protection &amp; Compliance","text":"<ul> <li>Regular system backups for critical user data only.</li> <li>Data encryption for sensitive research projects.</li> <li>User activity logging for compliance and audit purposes.</li> </ul>"},{"location":"resources/network/#future-enhancements","title":"Future Enhancements","text":"<p>TU HPC aims to improve networking and security infrastructure by:</p> <ul> <li>Implementing VPN-based secure access for external researchers.</li> <li>Expanding network bandwidth to support larger datasets and AI workloads.</li> <li>Enhancing intrusion detection systems (IDS) and automated monitoring.</li> </ul> <p>For more details on accessing the network securely, visit the Access &amp; Accounts page.</p> <p>Ensuring secure and efficient high-performance computing! \ud83d\ude80</p>"},{"location":"resources/software/","title":"Software & Tools","text":"<p>The HPC system at Tribhuvan University provides a diverse range of software tools and libraries to support research in computational physics, AI, molecular dynamics, and high-performance computing applications. The software environment is managed using environment modules, allowing users to load specific software versions as needed.</p>"},{"location":"resources/software/#available-software-modules","title":"Available Software &amp; Modules","text":"<p>Users can view the available software modules using the following command: <pre><code>module avail\n</code></pre> The following software and tools are currently installed on TU HPC:</p>"},{"location":"resources/software/#1-compilers-development-tools","title":"1. Compilers &amp; Development Tools","text":"<ul> <li>Intel Compiler (icc, ifort): <code>intel/2022.2</code>, <code>intel/2013</code></li> <li>GNU Compiler Collection (GCC): <code>gnu/5.4.0</code>, <code>gnu/8.3.0</code></li> <li>LLVM Compiler Infrastructure: <code>compiler-rt/2022.0.2</code></li> <li>CUDA Toolkit: <code>cuda/10.2</code>, <code>cuda/11.2</code></li> <li>OpenCL Development Tools: <code>oclfpga/2022.0.2</code></li> <li>CMake: <code>cmake/3.15.4</code></li> </ul>"},{"location":"resources/software/#2-mathematical-libraries","title":"2. Mathematical Libraries","text":"<ul> <li>FFTW (Fast Fourier Transform): <code>fftw/3.3.10</code>, <code>fftw/3.3.10_Intel</code></li> <li>LAPACK (Linear Algebra Package): <code>lapack/3.10.0</code></li> <li>Intel MKL (Math Kernel Library): <code>mkl/2013</code>, <code>mkl/2022.0.2</code></li> <li>TBB (Threading Building Blocks): <code>tbb/2021.5.1</code></li> </ul>"},{"location":"resources/software/#3-scientific-hpc-applications","title":"3. Scientific &amp; HPC Applications","text":"<ul> <li>Quantum Espresso (QE): <code>qe/7.0</code>, <code>qe/7.0-gpu</code>, <code>qe/7.2</code></li> <li>FPLO (Full-Potential Local-Orbital Code): <code>fplo/18.52</code></li> <li>GROMACS (Molecular Dynamics): <code>gromacs/2019.6</code>, <code>gromacs/2019.6-gpu</code></li> <li>NAMD (Molecular Dynamics): <code>namd/2.14</code></li> <li>Thermo_pw (Quantum Thermodynamics): <code>thermo_pw/1.7.0</code></li> </ul>"},{"location":"resources/software/#4-parallel-computing-mpi-implementations","title":"4. Parallel Computing &amp; MPI Implementations","text":"<ul> <li>OpenMPI: <code>openmpi/4.1.0</code>, <code>openmpi/4.1.2_Intel</code>, <code>openmpi/2022.2</code></li> <li>Intel MPI: <code>impi/4.1.3</code></li> <li>PMIx (Process Management Interface for Exascale Computing): <code>pmix/2.2.2</code></li> </ul>"},{"location":"resources/software/#5-python-ai-frameworks","title":"5. Python &amp; AI Frameworks","text":"<ul> <li>Python: <code>python/3.10.12</code>, <code>python/3.12.7</code></li> <li>Debugger &amp; Profiling Tools: <code>debugger/2021.5.0</code></li> <li>Data Parallel C++ Toolkit (DPC++): <code>dpct/2022.0.0</code></li> <li>Data Parallel Library (DPL): <code>dpl/2021.6.0</code></li> </ul>"},{"location":"resources/software/#using-modules-on-tu-hpc","title":"Using Modules on TU HPC","text":"<p>To use any software, load the corresponding module using: <pre><code>module load &lt;module-name&gt;\n</code></pre> For example, to load Python 3.12.7: <pre><code>module load python/3.12.7\n</code></pre> To check currently loaded modules: <pre><code>module list\n</code></pre> To unload a module: <pre><code>module unload &lt;module-name&gt;\n</code></pre></p>"},{"location":"resources/software/#software-installation-requests","title":"Software Installation Requests","text":"<p>If users require additional software, they can request installations by contacting HPC support. </p> <p>For more information on using software efficiently, visit the HPC User Guide.</p> <p>Optimized software environment for high-performance computing! \ud83d\ude80</p>"},{"location":"resources/storage/","title":"Storage Details","text":""},{"location":"resources/storage/#storage-tiers-configuration","title":"Storage Tiers &amp; Configuration","text":"<p>The TU HPC storage infrastructure is divided into multiple storage tiers, each designed for specific use cases:</p>"},{"location":"resources/storage/#1-software-storage","title":"1. Software Storage","text":"<ul> <li>Location: <code>/dev/sdb1</code></li> <li>Capacity: 470GB</li> <li>Purpose: Storing installed software, modules, and system libraries.</li> <li>Access: Read access for all users, write access for administrators.</li> </ul>"},{"location":"resources/storage/#2-scratch-storage-high-speed-temporary-storage","title":"2. Scratch Storage (High-Speed Temporary Storage)","text":"<ul> <li>Location: <code>/mnt/storage0</code></li> <li>Capacity: 1.9TB</li> <li>Purpose: High-speed temporary storage for running computations.</li> <li>Policy:</li> <li>Non-persistent: Data may be deleted after job completion.</li> <li>Users must regularly move important data to home or project storage.</li> </ul>"},{"location":"resources/storage/#3-home-directory-user-personal-storage","title":"3. Home Directory (User Personal Storage)","text":"<ul> <li>Location: <code>/home</code></li> <li>Capacity: 1.8TB</li> <li>Purpose: User-specific data, research scripts, personal files.</li> <li>Quota: Users have a designated storage limit to ensure fair use.</li> <li>Backup: Take your own backup.</li> </ul>"},{"location":"resources/storage/#4-memory-file-system-temporary-in-memory-storage","title":"4. Memory File System (Temporary In-Memory Storage)","text":"<ul> <li>Location: <code>tmpfs</code></li> <li>Capacity: 126GB</li> <li>Purpose: High-speed memory-based temporary storage.</li> <li>Use Cases: Best for temporary caching and intermediate data processing.</li> </ul>"},{"location":"resources/storage/#data-management-policies","title":"Data Management Policies","text":""},{"location":"resources/storage/#storage-usage-guidelines","title":"Storage Usage Guidelines","text":"<ul> <li>Scratch Storage (<code>/mnt/storage0</code>) should be used for active computations only, which can be also assessed inside (<code>/home/&lt;user-name&gt;/storage0</code>).</li> <li>Long-term storage should be maintained in the home directory (<code>/home</code>).</li> <li>Users must monitor their storage usage and clean up unnecessary files.</li> </ul>"},{"location":"resources/storage/#backup-data-security","title":"Backup &amp; Data Security","text":"<ul> <li>Home directories are not regularly backed up.</li> <li>Scratch storage is NOT backed up \u2013 users should save important data elsewhere.</li> <li>Users are responsible for managing their research data securely.</li> </ul>"},{"location":"resources/storage/#access-permissions","title":"Access &amp; Permissions","text":"<ul> <li>Software storage (<code>/software</code>) is managed by administrators.</li> <li>User files in <code>/home</code> are private but can be shared with appropriate permissions.</li> <li>Group storage options available for collaborative research projects.</li> </ul>"},{"location":"resources/storage/#future-storage-expansion","title":"Future Storage Expansion","text":"<p>TU HPC is continuously upgrading its storage infrastructure to support:</p> <ul> <li>Larger storage volumes for big data research.</li> <li>Faster access speeds for computationally intensive workloads.</li> <li>Enhanced data redundancy and recovery mechanisms.</li> </ul> <p>For details on managing your data efficiently, visit the Data Management Guide.</p> <p>Optimized storage solutions for high-performance computing! \ud83d\ude80</p>"}]}